From e8e7686786811bbd0d8d1950c4107dde3fed161e Mon Sep 17 00:00:00 2001
From: Huacai Chen <chenhc@lemote.com>
Date: Wed, 12 Aug 2020 18:09:42 +0800
Subject: [PATCH 126/131] MIPS: Add Loongson Advanced SIMD Extension (ASX)
 support

Signed-off-by: Huacai Chen <chenhc@lemote.com>
---
 arch/mips/Kconfig                    |  12 ++
 arch/mips/include/asm/asmmacro.h     |  53 +++++++
 arch/mips/include/asm/cpu-features.h |   6 +
 arch/mips/include/asm/cpu.h          |   1 +
 arch/mips/include/asm/fpu.h          |   6 +-
 arch/mips/include/asm/kvm_host.h     |  24 ++-
 arch/mips/include/asm/mipsregs.h     |   5 +
 arch/mips/include/asm/msa.h          |  83 +++++++++-
 arch/mips/include/asm/processor.h    |   9 +-
 arch/mips/include/asm/thread_info.h  |   2 +
 arch/mips/include/uapi/asm/kvm.h     |   1 +
 arch/mips/kernel/cpu-probe.c         |   8 +
 arch/mips/kernel/proc.c              |   1 +
 arch/mips/kernel/process.c           |   6 +-
 arch/mips/kernel/ptrace.c            |  31 ++++
 arch/mips/kernel/r4k_fpu.S           | 142 +++++++++++++++++
 arch/mips/kernel/traps.c             |  56 +++++--
 arch/mips/kvm/Makefile               |   1 +
 arch/mips/kvm/asx.S                  | 189 +++++++++++++++++++++++
 arch/mips/kvm/entry.c                |   4 +
 arch/mips/kvm/mips.c                 | 220 +++++++++++++++++++++++++--
 arch/mips/kvm/trace.h                |   6 +-
 arch/mips/kvm/vz.c                   |  55 ++++++-
 include/uapi/linux/elf.h             |   1 +
 include/uapi/linux/kvm.h             |   1 +
 25 files changed, 886 insertions(+), 37 deletions(-)
 create mode 100644 arch/mips/kvm/asx.S

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 29d32f8885aa..83c507f7c82c 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -1403,6 +1403,7 @@ config CPU_LOONGSON3
 	select CPU_SUPPORTS_HIGHMEM
 	select CPU_SUPPORTS_HUGEPAGES
 	select CPU_SUPPORTS_MSA
+	select CPU_SUPPORTS_ASX
 	select CPU_HAS_LOAD_STORE_LR
 	select WEAK_ORDERING
 	select WEAK_REORDERING_BEYOND_LLSC
@@ -2556,6 +2557,14 @@ config CPU_HAS_MSA
 
 	  If unsure, say Y.
 
+config CPU_HAS_ASX
+	bool "Support for the Loongson Advanced SIMD Extension"
+	depends on CPU_SUPPORTS_ASX
+	depends on 64BIT && CPU_HAS_MSA
+	help
+	  Loongson Advanced SIMD Extension (ASX) is 256 bit wide SIMD extension
+	  first introduced in Loongson-3A4000. It is compatible with MSA.
+
 config CPU_HAS_WB
 	bool
 
@@ -2658,6 +2667,9 @@ config SYS_SUPPORTS_MIPS16
 config CPU_SUPPORTS_MSA
 	bool
 
+config CPU_SUPPORTS_ASX
+	bool
+
 config ARCH_FLATMEM_ENABLE
 	def_bool y
 	depends on !NUMA && !CPU_LOONGSON2
diff --git a/arch/mips/include/asm/asmmacro.h b/arch/mips/include/asm/asmmacro.h
index feb069cbf44e..fa3afb0cf6fb 100644
--- a/arch/mips/include/asm/asmmacro.h
+++ b/arch/mips/include/asm/asmmacro.h
@@ -506,6 +506,16 @@
 	.endm
 #endif
 
+#ifdef CONFIG_CPU_HAS_MSA
+	.macro	cfmsa1
+	.word	0x787e0059 | (1 << 11)
+	.endm
+
+	.macro	ctmsa1
+	.word	0x783e0819 | (1 << 6)
+	.endm
+#endif
+
 #ifdef TOOLCHAIN_SUPPORTS_MSA
 #define FPR_BASE_OFFS	THREAD_FPR0
 #define FPR_BASE	$1
@@ -654,4 +664,47 @@
 	.set	pop
 	.endm
 
+/*
+ * Helper macros for Loongson ASX instruction encodings.
+ */
+	.macro	xvld_b	wd, off, base
+	.set	push
+	.set	noat
+	SET_HARDFLOAT
+	PTR_ADDU $1, \base, \off
+	insn_if_mips 0xc8000819 | (\wd << 6)
+	.set	pop
+	.endm
+
+	.macro	xvst_b	wd, off, base
+	.set	push
+	.set	noat
+	SET_HARDFLOAT
+	PTR_ADDU $1, \base, \off
+	insn_if_mips 0xe8000819 | (\wd << 6)
+	.set	pop
+	.endm
+
+	.macro xinsert_d wd, n
+	.set	push
+	.set	noat
+	SET_HARDFLOAT
+	insn_if_mips 0x79380819 | (\n << 16) | (\wd << 6)
+	.set	pop
+	.endm
+
+	.macro	xvseli_d patt, ws, wd
+	.set	push
+	.set	noat
+	.word	0xed00000a | (\wd << 6) | (\ws << 11) | (\patt << 16)
+	.set	pop
+	.endm
+
+	.macro	asx_init_upper wd
+	.set	push
+	.set	noat
+	xinsert_d \wd, 2
+	xinsert_d \wd, 3
+	.endm
+
 #endif /* _ASM_ASMMACRO_H */
diff --git a/arch/mips/include/asm/cpu-features.h b/arch/mips/include/asm/cpu-features.h
index 8704209c8fb0..7604e19d241c 100644
--- a/arch/mips/include/asm/cpu-features.h
+++ b/arch/mips/include/asm/cpu-features.h
@@ -514,6 +514,12 @@
 # define cpu_has_msa		0
 #endif
 
+#if defined(CONFIG_CPU_HAS_ASX) && !defined(cpu_has_asx)
+# define cpu_has_asx		__ase(MIPS_ASE_LOONGSON_ASX)
+#elif !defined(cpu_has_asx)
+# define cpu_has_asx		0
+#endif
+
 #ifndef cpu_has_ufr
 # define cpu_has_ufr		__opt(MIPS_CPU_UFR)
 #endif
diff --git a/arch/mips/include/asm/cpu.h b/arch/mips/include/asm/cpu.h
index 530e42d872a6..8396ca0dda9a 100644
--- a/arch/mips/include/asm/cpu.h
+++ b/arch/mips/include/asm/cpu.h
@@ -437,5 +437,6 @@ enum cpu_type_enum {
 #define MIPS_ASE_LOONGSON_CAM	0x00001000 /* Loongson CAM */
 #define MIPS_ASE_LOONGSON_EXT	0x00002000 /* Loongson EXTensions */
 #define MIPS_ASE_LOONGSON_EXT2	0x00004000 /* Loongson EXTensions R2 */
+#define MIPS_ASE_LOONGSON_ASX	0x00008000 /* Loongson Advanced SIMD Extension */
 
 #endif /* _ASM_CPU_H */
diff --git a/arch/mips/include/asm/fpu.h b/arch/mips/include/asm/fpu.h
index 9476e0498d59..8cc610eeadaa 100644
--- a/arch/mips/include/asm/fpu.h
+++ b/arch/mips/include/asm/fpu.h
@@ -165,10 +165,14 @@ static inline void lose_fpu_inatomic(int save, struct task_struct *tsk)
 {
 	if (is_msa_enabled()) {
 		if (save) {
-			save_msa(tsk);
+			if (is_asx_enabled())
+				save_asx(tsk);
+			else
+				save_msa(tsk);
 			tsk->thread.fpu.fcr31 =
 					read_32bit_cp1_register(CP1_STATUS);
 		}
+		disable_asx();
 		disable_msa();
 		clear_tsk_thread_flag(tsk, TIF_USEDMSA);
 		__disable_fpu();
diff --git a/arch/mips/include/asm/kvm_host.h b/arch/mips/include/asm/kvm_host.h
index c4f43ef6c998..6ec303d00fb9 100644
--- a/arch/mips/include/asm/kvm_host.h
+++ b/arch/mips/include/asm/kvm_host.h
@@ -165,6 +165,7 @@ struct kvm_vcpu_stat {
 	u64 msa_fpe_exits;
 	u64 fpe_exits;
 	u64 msa_disabled_exits;
+	u64 asx_disabled_exits;
 	u64 flush_dcache_exits;
 #ifdef CONFIG_KVM_MIPS_VZ
 	u64 vz_gpsi_exits;
@@ -346,6 +347,7 @@ struct kvm_mmu_memory_cache {
 
 #define KVM_MIPS_AUX_FPU	0x1
 #define KVM_MIPS_AUX_MSA	0x2
+#define KVM_MIPS_AUX_ASX	0x4
 
 #define KVM_MIPS_GUEST_TLB_SIZE	64
 struct kvm_vcpu_arch {
@@ -365,6 +367,7 @@ struct kvm_vcpu_arch {
 	u32 host_cp0_guestctl0;
 	u32 host_cp0_badinstr;
 	u32 host_cp0_badinstrp;
+	u32 host_cp0_diag1;
 
 	/* GPRS */
 	unsigned long gprs[32];
@@ -443,6 +446,7 @@ struct kvm_vcpu_arch {
 
 	u8 fpu_enabled;
 	u8 msa_enabled;
+	u8 asx_enabled;
 };
 
 static inline void _kvm_atomic_set_c0_guest_reg(unsigned long *reg,
@@ -804,6 +808,18 @@ static inline bool kvm_mips_guest_has_msa(struct kvm_vcpu_arch *vcpu)
 		kvm_read_c0_guest_config3(vcpu->cop0) & MIPS_CONF3_MSA;
 }
 
+static inline bool kvm_mips_guest_can_have_asx(struct kvm_vcpu_arch *vcpu)
+{
+	return (!__builtin_constant_p(cpu_has_asx) || cpu_has_asx) &&
+		vcpu->asx_enabled;
+}
+
+static inline bool kvm_mips_guest_has_asx(struct kvm_vcpu_arch *vcpu)
+{
+	return kvm_mips_guest_can_have_asx(vcpu) &&
+		kvm_read_sw_gc0_config6(vcpu->cop0) & MIPS_CONF6_LASXMODE;
+}
+
 struct kvm_mips_callbacks {
 	int (*handle_cop_unusable)(struct kvm_vcpu *vcpu);
 	int (*handle_tlb_mod)(struct kvm_vcpu *vcpu);
@@ -818,6 +834,7 @@ struct kvm_mips_callbacks {
 	int (*handle_msa_fpe)(struct kvm_vcpu *vcpu);
 	int (*handle_fpe)(struct kvm_vcpu *vcpu);
 	int (*handle_msa_disabled)(struct kvm_vcpu *vcpu);
+	int (*handle_asx_disabled)(struct kvm_vcpu *vcpu);
 	int (*handle_guest_exit)(struct kvm_vcpu *vcpu);
 	int (*hardware_enable)(void);
 	void (*hardware_disable)(void);
@@ -870,16 +887,21 @@ void *kvm_mips_build_tlb_refill_exception(void *addr, void *handler);
 void *kvm_mips_build_exception(void *addr, void *handler);
 void *kvm_mips_build_exit(void *addr);
 
-/* FPU/MSA context management */
+/* FPU/MSA/ASX context management */
 void __kvm_save_fpu(struct kvm_vcpu_arch *vcpu);
 void __kvm_restore_fpu(struct kvm_vcpu_arch *vcpu);
 void __kvm_restore_fcsr(struct kvm_vcpu_arch *vcpu);
 void __kvm_save_msa(struct kvm_vcpu_arch *vcpu);
 void __kvm_restore_msa(struct kvm_vcpu_arch *vcpu);
 void __kvm_restore_msa_upper(struct kvm_vcpu_arch *vcpu);
+void __kvm_save_asx(struct kvm_vcpu_arch *vcpu);
+void __kvm_restore_asx(struct kvm_vcpu_arch *vcpu);
+void __kvm_restore_asx_upper128(struct kvm_vcpu_arch *vcpu);
+void __kvm_restore_asx_upper192(struct kvm_vcpu_arch *vcpu);
 void __kvm_restore_msacsr(struct kvm_vcpu_arch *vcpu);
 void kvm_own_fpu(struct kvm_vcpu *vcpu);
 void kvm_own_msa(struct kvm_vcpu *vcpu);
+void kvm_own_asx(struct kvm_vcpu *vcpu);
 void kvm_drop_fpu(struct kvm_vcpu *vcpu);
 void kvm_lose_fpu(struct kvm_vcpu *vcpu);
 
diff --git a/arch/mips/include/asm/mipsregs.h b/arch/mips/include/asm/mipsregs.h
index e0e21e524411..db052d898212 100644
--- a/arch/mips/include/asm/mipsregs.h
+++ b/arch/mips/include/asm/mipsregs.h
@@ -568,6 +568,7 @@
 #define MIPS_CONF_MT_FTLB	(_ULCAST_(4) <<  7)
 #define MIPS_CONF_AR		(_ULCAST_(7) << 10)
 #define MIPS_CONF_AT		(_ULCAST_(3) << 13)
+#define MIPS_CONF_LASXEN	(_ULCAST_(1) << 19)
 #define MIPS_CONF_M		(_ULCAST_(1) << 31)
 
 /*
@@ -685,6 +686,8 @@
 #define MIPS_CONF6_SYND		(_ULCAST_(1) << 13)
 /* proAptiv FTLB on/off bit */
 #define MIPS_CONF6_FTLBEN	(_ULCAST_(1) << 15)
+/* Loongson-3's indicator of ASX mode */
+#define MIPS_CONF6_LASXMODE	(_ULCAST_(1) << 21)
 /* Loongson-3 FTLB on/off bit */
 #define MIPS_CONF6_FTLBDIS	(_ULCAST_(1) << 22)
 /* FTLB probability bits */
@@ -1015,6 +1018,8 @@
 /* Loongson-specific exception code (GSExcCode) */
 #define LOONGSON_DIAG1_EXCCODE_SHIFT	2
 #define LOONGSON_DIAG1_EXCCODE		GENMASK(6, 2)
+#define  GSEXCCODE_LASXDIS		7
+#define  GSEXCCODE_LBTDIS		8
 
 /* CvmCtl register field definitions */
 #define CVMCTL_IPPCI_SHIFT	7
diff --git a/arch/mips/include/asm/msa.h b/arch/mips/include/asm/msa.h
index e0a3dd52334d..a198eab108ee 100644
--- a/arch/mips/include/asm/msa.h
+++ b/arch/mips/include/asm/msa.h
@@ -12,10 +12,21 @@
 
 #include <asm/inst.h>
 
+enum {
+	CTX_MSA = 1,
+	CTX_ASX = 2,
+};
+
 extern void _save_msa(struct task_struct *);
 extern void _restore_msa(struct task_struct *);
 extern void _init_msa_upper(void);
 
+#ifdef CONFIG_CPU_HAS_ASX
+extern void _save_asx(struct task_struct *);
+extern void _restore_asx(struct task_struct *);
+extern void _init_asx_upper(void);
+#endif
+
 extern void read_msa_wr_b(unsigned idx, union fpureg *to);
 extern void read_msa_wr_h(unsigned idx, union fpureg *to);
 extern void read_msa_wr_w(unsigned idx, union fpureg *to);
@@ -120,15 +131,25 @@ static inline int is_msa_enabled(void)
 
 static inline int thread_msa_context_live(void)
 {
+	int ret = 0;
+
 	/*
 	 * Check cpu_has_msa only if it's a constant. This will allow the
 	 * compiler to optimise out code for CPUs without MSA without adding
 	 * an extra redundant check for CPUs with MSA.
 	 */
 	if (__builtin_constant_p(cpu_has_msa) && !cpu_has_msa)
-		return 0;
+		goto out;
 
-	return test_thread_flag(TIF_MSA_CTX_LIVE);
+	ret = test_thread_flag(TIF_MSA_CTX_LIVE) ? CTX_MSA : 0;
+
+	if (__builtin_constant_p(cpu_has_asx) && !cpu_has_asx)
+		goto out;
+
+	ret = test_thread_flag(TIF_ASX_CTX_LIVE) ? CTX_ASX : ret;
+
+out:
+	return ret;
 }
 
 static inline void save_msa(struct task_struct *t)
@@ -156,6 +177,64 @@ static inline void init_msa_upper(void)
 	_init_msa_upper();
 }
 
+#ifdef CONFIG_CPU_HAS_ASX
+
+static inline void enable_asx(void)
+{
+	if (cpu_has_asx)
+		set_c0_config(MIPS_CONF_LASXEN);
+}
+
+static inline void disable_asx(void)
+{
+	if (cpu_has_asx)
+		clear_c0_config(MIPS_CONF_LASXEN);
+}
+
+static inline int is_asx_enabled(void)
+{
+	if (!cpu_has_asx)
+		return 0;
+
+	return (read_c0_config() & MIPS_CONF_LASXEN);
+}
+
+static inline void save_asx(struct task_struct *t)
+{
+	if (cpu_has_asx)
+		_save_asx(t);
+}
+
+static inline void restore_asx(struct task_struct *t)
+{
+	if (cpu_has_asx)
+		_restore_asx(t);
+}
+
+static inline void init_asx_upper(void)
+{
+	/*
+	 * Check cpu_has_asx only if it's a constant. This will allow the
+	 * compiler to optimise out code for CPUs without ASX without adding
+	 * an extra redundant check for CPUs with ASX.
+	 */
+	if (__builtin_constant_p(cpu_has_asx) && !cpu_has_asx)
+		return;
+
+	_init_asx_upper();
+}
+
+#else
+
+static inline void enable_asx(void) {}
+static inline void disable_asx(void) {}
+static inline int is_asx_enabled(void) { return 0; }
+static inline void save_asx(struct task_struct *t) {}
+static inline void restore_asx(struct task_struct *t) {}
+static inline void init_asx_upper(void) {}
+
+#endif
+
 #ifndef TOOLCHAIN_SUPPORTS_MSA
 /*
  * Define assembler macros using .word for the c[ft]cmsa instructions in order
diff --git a/arch/mips/include/asm/processor.h b/arch/mips/include/asm/processor.h
index 7fc4020876e0..1572bfb97d6f 100644
--- a/arch/mips/include/asm/processor.h
+++ b/arch/mips/include/asm/processor.h
@@ -85,7 +85,9 @@ extern unsigned long mips_stack_top(void);
 
 #define NUM_FPU_REGS	32
 
-#ifdef CONFIG_CPU_HAS_MSA
+#if defined(CONFIG_CPU_HAS_ASX)
+# define FPU_REG_WIDTH	256
+#elif defined(CONFIG_CPU_HAS_MSA)
 # define FPU_REG_WIDTH	128
 #else
 # define FPU_REG_WIDTH	64
@@ -228,7 +230,10 @@ typedef struct {
 	unsigned long seg;
 } mm_segment_t;
 
-#ifdef CONFIG_CPU_HAS_MSA
+#if defined(CONFIG_CPU_HAS_ASX)
+# define ARCH_MIN_TASKALIGN	32
+# define FPU_ALIGN		__aligned(32)
+#elif defined(CONFIG_CPU_HAS_MSA)
 # define ARCH_MIN_TASKALIGN	16
 # define FPU_ALIGN		__aligned(16)
 #else
diff --git a/arch/mips/include/asm/thread_info.h b/arch/mips/include/asm/thread_info.h
index ee26f9a4575d..fb8f44b419de 100644
--- a/arch/mips/include/asm/thread_info.h
+++ b/arch/mips/include/asm/thread_info.h
@@ -117,6 +117,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_UPROBE		6	/* breakpointed or singlestepping */
 #define TIF_RESTORE_SIGMASK	9	/* restore signal mask in do_signal() */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
+#define TIF_ASX_CTX_LIVE	17	/* ASX context must be preserved */
 #define TIF_MEMDIE		18	/* is terminating due to OOM killer */
 #define TIF_NOHZ		19	/* in adaptive nohz mode */
 #define TIF_FIXADE		20	/* Fix address errors in software */
@@ -151,6 +152,7 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_HYBRID_FPREGS	(1<<TIF_HYBRID_FPREGS)
 #define _TIF_USEDMSA		(1<<TIF_USEDMSA)
 #define _TIF_MSA_CTX_LIVE	(1<<TIF_MSA_CTX_LIVE)
+#define _TIF_ASX_CTX_LIVE	(1<<TIF_ASX_CTX_LIVE)
 #define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)
 
 #define _TIF_WORK_SYSCALL_ENTRY	(_TIF_NOHZ | _TIF_SYSCALL_TRACE |	\
diff --git a/arch/mips/include/uapi/asm/kvm.h b/arch/mips/include/uapi/asm/kvm.h
index edcf717c4327..c1aee0d9cedb 100644
--- a/arch/mips/include/uapi/asm/kvm.h
+++ b/arch/mips/include/uapi/asm/kvm.h
@@ -184,6 +184,7 @@ struct kvm_fpu {
 #define KVM_REG_MIPS_FPR_32(n)	(KVM_REG_MIPS_FPR | KVM_REG_SIZE_U32  | (n))
 #define KVM_REG_MIPS_FPR_64(n)	(KVM_REG_MIPS_FPR | KVM_REG_SIZE_U64  | (n))
 #define KVM_REG_MIPS_VEC_128(n)	(KVM_REG_MIPS_FPR | KVM_REG_SIZE_U128 | (n))
+#define KVM_REG_MIPS_VEC_256(n)	(KVM_REG_MIPS_FPR | KVM_REG_SIZE_U256 | (n))
 
 /*
  * KVM_REG_MIPS_FCR - Floating point control registers.
diff --git a/arch/mips/kernel/cpu-probe.c b/arch/mips/kernel/cpu-probe.c
index 04d3f7ba5e16..8e57a135b9fc 100644
--- a/arch/mips/kernel/cpu-probe.c
+++ b/arch/mips/kernel/cpu-probe.c
@@ -1935,6 +1935,14 @@ static void decode_loongson_cpucfg(struct cpuinfo_mips *c)
 #ifdef CONFIG_CPU_LOONGSON3
 	unsigned int cpucfg;
 
+#ifdef CONFIG_CPU_HAS_ASX
+	cpucfg = read_cpucfg(LOONGSON_CFG1);
+	if (cpucfg & LOONGSON_CFG1_LASX) {
+		c->ases |= MIPS_ASE_LOONGSON_ASX;
+		set_c0_config6(MIPS_CONF6_LASXMODE);
+	}
+#endif
+
 	cpucfg = read_cpucfg(LOONGSON_CFG2);
 	if (cpucfg & LOONGSON_CFG2_LLFTP)
 		c->options |= MIPS_CPU_CONST_TIMER;
diff --git a/arch/mips/kernel/proc.c b/arch/mips/kernel/proc.c
index 97b66edab160..40005e440164 100644
--- a/arch/mips/kernel/proc.c
+++ b/arch/mips/kernel/proc.c
@@ -130,6 +130,7 @@ static int show_cpuinfo(struct seq_file *m, void *v)
 	if (cpu_has_eva)	seq_printf(m, "%s", " eva");
 	if (cpu_has_htw)	seq_printf(m, "%s", " htw");
 	if (cpu_has_xpa)	seq_printf(m, "%s", " xpa");
+	if (cpu_has_asx)	seq_printf(m, "%s", " loongson-asx");
 	if (cpu_has_loongson_mmi)	seq_printf(m, "%s", " loongson-mmi");
 	if (cpu_has_loongson_cam)	seq_printf(m, "%s", " loongson-cam");
 	if (cpu_has_loongson_ext)	seq_printf(m, "%s", " loongson-ext");
diff --git a/arch/mips/kernel/process.c b/arch/mips/kernel/process.c
index 958faa51dfba..3f18d7e31f4b 100644
--- a/arch/mips/kernel/process.c
+++ b/arch/mips/kernel/process.c
@@ -74,6 +74,7 @@ void start_thread(struct pt_regs * regs, unsigned long pc, unsigned long sp)
 	regs->cp0_status = status;
 	lose_fpu(0);
 	clear_thread_flag(TIF_MSA_CTX_LIVE);
+	clear_thread_flag(TIF_ASX_CTX_LIVE);
 	clear_used_math();
 	atomic_set(&current->thread.bd_emu_frame, BD_EMUFRAME_NONE);
 	init_dsp();
@@ -108,7 +109,9 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	 */
 	preempt_disable();
 
-	if (is_msa_enabled())
+	if (is_asx_enabled())
+		save_asx(current);
+	else if (is_msa_enabled())
 		save_msa(current);
 	else if (is_fpu_owner())
 		_save_fp(current);
@@ -177,6 +180,7 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long usp,
 	clear_tsk_thread_flag(p, TIF_USEDFPU);
 	clear_tsk_thread_flag(p, TIF_USEDMSA);
 	clear_tsk_thread_flag(p, TIF_MSA_CTX_LIVE);
+	clear_tsk_thread_flag(p, TIF_ASX_CTX_LIVE);
 
 #ifdef CONFIG_MIPS_MT_FPAFF
 	clear_tsk_thread_flag(p, TIF_FPUBOUND);
diff --git a/arch/mips/kernel/ptrace.c b/arch/mips/kernel/ptrace.c
index 414b6e9c900b..87ff4b3f06c6 100644
--- a/arch/mips/kernel/ptrace.c
+++ b/arch/mips/kernel/ptrace.c
@@ -685,6 +685,10 @@ static int msa_get(struct task_struct *target,
 		/* Copy scalar FP context, fill the rest with 0xff */
 		err = copy_pad_fprs(target, regset, &pos, &count,
 				    &kbuf, &ubuf, 8);
+	} else if (!test_tsk_thread_flag(target, TIF_ASX_CTX_LIVE)) {
+		/* Copy classic MSA context, fill the rest with 0xff */
+		err = copy_pad_fprs(target, regset, &pos, &count,
+				    &kbuf, &ubuf, 16);
 	} else if (sizeof(target->thread.fpu.fpr[0]) == regset->size) {
 		/* Trivially copy the vector registers */
 		err = user_regset_copyout(&pos, &count, &kbuf, &ubuf,
@@ -739,6 +743,10 @@ static int msa_set(struct task_struct *target,
 	if (!err) {
 		target->thread.fpu.fcr31 = ctrl_regs.fcsr & ~FPU_CSR_ALL_X;
 		target->thread.fpu.msacsr = ctrl_regs.msacsr & ~MSA_CSR_CAUSEF;
+		if (regset->size >= 16)
+			set_tsk_thread_flag(target, TIF_MSA_CTX_LIVE);
+		if (regset->size == 32)
+			set_tsk_thread_flag(target, TIF_ASX_CTX_LIVE);
 	}
 
 	return err;
@@ -925,6 +933,9 @@ enum mips_regset {
 #ifdef CONFIG_CPU_HAS_MSA
 	REGSET_MSA,
 #endif
+#ifdef CONFIG_CPU_HAS_ASX
+	REGSET_ASX,
+#endif
 };
 
 struct pt_regs_offset {
@@ -1059,6 +1070,16 @@ static const struct user_regset mips_regsets[] = {
 		.set		= msa_set,
 	},
 #endif
+#ifdef CONFIG_CPU_HAS_ASX
+	[REGSET_ASX] = {
+		.core_note_type	= NT_MIPS_ASX,
+		.n		= NUM_FPU_REGS + 1,
+		.size		= 32,
+		.align		= 32,
+		.get		= msa_get,
+		.set		= msa_set,
+	},
+#endif
 };
 
 static const struct user_regset_view user_mips_view = {
@@ -1119,6 +1140,16 @@ static const struct user_regset mips64_regsets[] = {
 		.set		= msa_set,
 	},
 #endif
+#ifdef CONFIG_CPU_HAS_ASX
+	[REGSET_ASX] = {
+		.core_note_type	= NT_MIPS_ASX,
+		.n		= NUM_FPU_REGS + 1,
+		.size		= 32,
+		.align		= 32,
+		.get		= msa_get,
+		.set		= msa_set,
+	},
+#endif
 };
 
 static const struct user_regset_view user_mips64_view = {
diff --git a/arch/mips/kernel/r4k_fpu.S b/arch/mips/kernel/r4k_fpu.S
index 59be5c812aa2..86325ef5e963 100644
--- a/arch/mips/kernel/r4k_fpu.S
+++ b/arch/mips/kernel/r4k_fpu.S
@@ -408,6 +408,148 @@ LEAF(_restore_msa_all_upper)
 
 #endif /* CONFIG_CPU_HAS_MSA */
 
+#ifdef CONFIG_CPU_HAS_ASX
+
+/*
+ * Save a thread's ASX vector context.
+ */
+LEAF(_save_asx)
+EXPORT_SYMBOL(_save_asx)
+	.set	push
+	.set	noat
+	.set	noreorder
+	.set	msa
+	SET_HARDFLOAT
+	cfmsa1
+	sw	$1, THREAD_MSA_CSR(a0)
+	xvst_b	0, THREAD_FPR0, a0
+	xvst_b	1, THREAD_FPR1, a0
+	xvst_b	2, THREAD_FPR2, a0
+	xvst_b	3, THREAD_FPR3, a0
+	xvst_b	4, THREAD_FPR4, a0
+	xvst_b	5, THREAD_FPR5, a0
+	xvst_b	6, THREAD_FPR6, a0
+	xvst_b	7, THREAD_FPR7, a0
+	xvst_b	8, THREAD_FPR8, a0
+	xvst_b	9, THREAD_FPR9, a0
+	xvst_b	10, THREAD_FPR10, a0
+	xvst_b	11, THREAD_FPR11, a0
+	xvst_b	12, THREAD_FPR12, a0
+	xvst_b	13, THREAD_FPR13, a0
+	xvst_b	14, THREAD_FPR14, a0
+	xvst_b	15, THREAD_FPR15, a0
+	xvst_b	16, THREAD_FPR16, a0
+	xvst_b	17, THREAD_FPR17, a0
+	xvst_b	18, THREAD_FPR18, a0
+	xvst_b	19, THREAD_FPR19, a0
+	xvst_b	20, THREAD_FPR20, a0
+	xvst_b	21, THREAD_FPR21, a0
+	xvst_b	22, THREAD_FPR22, a0
+	xvst_b	23, THREAD_FPR23, a0
+	xvst_b	24, THREAD_FPR24, a0
+	xvst_b	25, THREAD_FPR25, a0
+	xvst_b	26, THREAD_FPR26, a0
+	xvst_b	27, THREAD_FPR27, a0
+	xvst_b	28, THREAD_FPR28, a0
+	xvst_b	29, THREAD_FPR29, a0
+	xvst_b	30, THREAD_FPR30, a0
+	xvst_b	31, THREAD_FPR31, a0
+	jr	ra
+	 nop
+END(_save_asx)
+
+/*
+ * Restore a thread's ASX vector context.
+ */
+LEAF(_restore_asx)
+	.set	push
+	.set	noat
+	.set	noreorder
+	.set	msa
+	SET_HARDFLOAT
+	lw	$1, THREAD_MSA_CSR(a0)
+	ctmsa1
+	.set	pop
+	xvld_b	0, THREAD_FPR0, a0
+	xvld_b	1, THREAD_FPR1, a0
+	xvld_b	2, THREAD_FPR2, a0
+	xvld_b	3, THREAD_FPR3, a0
+	xvld_b	4, THREAD_FPR4, a0
+	xvld_b	5, THREAD_FPR5, a0
+	xvld_b	6, THREAD_FPR6, a0
+	xvld_b	7, THREAD_FPR7, a0
+	xvld_b	8, THREAD_FPR8, a0
+	xvld_b	9, THREAD_FPR9, a0
+	xvld_b	10, THREAD_FPR10, a0
+	xvld_b	11, THREAD_FPR11, a0
+	xvld_b	12, THREAD_FPR12, a0
+	xvld_b	13, THREAD_FPR13, a0
+	xvld_b	14, THREAD_FPR14, a0
+	xvld_b	15, THREAD_FPR15, a0
+	xvld_b	16, THREAD_FPR16, a0
+	xvld_b	17, THREAD_FPR17, a0
+	xvld_b	18, THREAD_FPR18, a0
+	xvld_b	19, THREAD_FPR19, a0
+	xvld_b	20, THREAD_FPR20, a0
+	xvld_b	21, THREAD_FPR21, a0
+	xvld_b	22, THREAD_FPR22, a0
+	xvld_b	23, THREAD_FPR23, a0
+	xvld_b	24, THREAD_FPR24, a0
+	xvld_b	25, THREAD_FPR25, a0
+	xvld_b	26, THREAD_FPR26, a0
+	xvld_b	27, THREAD_FPR27, a0
+	xvld_b	28, THREAD_FPR28, a0
+	xvld_b	29, THREAD_FPR29, a0
+	xvld_b	30, THREAD_FPR30, a0
+	xvld_b	31, THREAD_FPR31, a0
+	jr	ra
+	 nop
+END(_restore_asx)
+
+LEAF(_init_asx_upper)
+	.set	push
+	.set	noat
+	.set	noreorder
+	SET_HARDFLOAT
+	not	$1, zero
+	asx_init_upper	0
+	xvseli_d 0xc, 0, 1
+	xvseli_d 0xc, 0, 2
+	xvseli_d 0xc, 0, 3
+	xvseli_d 0xc, 0, 4
+	xvseli_d 0xc, 0, 5
+	xvseli_d 0xc, 0, 6
+	xvseli_d 0xc, 0, 7
+	xvseli_d 0xc, 0, 8
+	xvseli_d 0xc, 0, 9
+	xvseli_d 0xc, 0, 10
+	xvseli_d 0xc, 0, 11
+	xvseli_d 0xc, 0, 12
+	xvseli_d 0xc, 0, 13
+	xvseli_d 0xc, 0, 14
+	xvseli_d 0xc, 0, 15
+	xvseli_d 0xc, 0, 16
+	xvseli_d 0xc, 0, 17
+	xvseli_d 0xc, 0, 18
+	xvseli_d 0xc, 0, 19
+	xvseli_d 0xc, 0, 20
+	xvseli_d 0xc, 0, 21
+	xvseli_d 0xc, 0, 22
+	xvseli_d 0xc, 0, 23
+	xvseli_d 0xc, 0, 24
+	xvseli_d 0xc, 0, 25
+	xvseli_d 0xc, 0, 26
+	xvseli_d 0xc, 0, 27
+	xvseli_d 0xc, 0, 28
+	xvseli_d 0xc, 0, 29
+	xvseli_d 0xc, 0, 30
+	jr	ra
+	 xvseli_d 0xc, 0, 31
+	.set	pop
+END(_init_asx_upper)
+
+#endif /* CONFIG_CPU_HAS_ASX */
+
 	.set	reorder
 
 	.type	fault, @function
diff --git a/arch/mips/kernel/traps.c b/arch/mips/kernel/traps.c
index 816910f68ab6..a602d34258fe 100644
--- a/arch/mips/kernel/traps.c
+++ b/arch/mips/kernel/traps.c
@@ -1312,18 +1312,13 @@ static int enable_restore_fp_context(int msa)
 	 * opportunity to see data left behind by another.
 	 */
 	prior_msa = test_and_set_thread_flag(TIF_MSA_CTX_LIVE);
-	if (!prior_msa && was_fpu_owner) {
-		init_msa_upper();
-
-		goto out;
-	}
-
 	if (!prior_msa) {
 		/*
 		 * Restore the least significant 64b of each vector register
 		 * from the existing scalar FP context.
 		 */
-		_restore_fp(current);
+		if (!was_fpu_owner)
+			_restore_fp(current);
 
 		/*
 		 * The task has not formerly used MSA, so clear the upper 64b
@@ -1332,13 +1327,18 @@ static int enable_restore_fp_context(int msa)
 		 */
 		init_msa_upper();
 	} else {
-		/* We need to restore the vector context. */
-		restore_msa(current);
-
 		/* Restore the scalar FP control & status register */
 		if (!was_fpu_owner)
 			write_32bit_cp1_register(CP1_STATUS,
 						 current->thread.fpu.fcr31);
+
+		/* We need to restore the vector context. */
+		if (!test_thread_flag(TIF_ASX_CTX_LIVE))
+			restore_msa(current);
+		else {
+			enable_asx();
+			restore_asx(current);
+		}
 	}
 
 out:
@@ -1878,13 +1878,37 @@ asmlinkage void do_gsexc(struct pt_regs *regs, u32 diag1)
 
 	prev_state = exception_enter();
 
+	die_if_kernel("do_gsexc invoked from kernel context!", regs);
+
 	switch (exccode) {
-	case 0x08:
-		/* Undocumented exception, will trigger on certain
-		 * also-undocumented instructions accessible from userspace.
-		 * Processor state is not otherwise corrupted, but currently
-		 * we don't know how to proceed. Maybe there is some
-		 * undocumented control flag to enable the instructions?
+	case GSEXCCODE_LASXDIS:
+		if (!cpu_has_asx || test_thread_flag(TIF_32BIT_FPREGS)) {
+			force_sig(SIGILL);
+			break;
+		}
+
+		preempt_disable();
+
+		set_thread_flag(TIF_ASX_CTX_LIVE);
+
+		if(!is_fpu_owner()) {
+			own_fpu_inatomic(0);
+			write_32bit_cp1_register(CP1_STATUS, current->thread.fpu.fcr31);
+			enable_msa();
+			restore_msa(current);
+			set_thread_flag(TIF_USEDMSA);
+		}
+
+		enable_asx();
+		init_asx_upper();
+
+		preempt_enable();
+
+		break;
+
+	case GSEXCCODE_LBTDIS:
+		/* LBT (Loongson's binary translation) disabled exception, will
+		 * trigger on certain LBT instructions accessible from userspace.
 		 */
 		force_sig(SIGILL);
 		break;
diff --git a/arch/mips/kvm/Makefile b/arch/mips/kvm/Makefile
index 884b8e81dd25..691409e8afd5 100644
--- a/arch/mips/kvm/Makefile
+++ b/arch/mips/kvm/Makefile
@@ -7,6 +7,7 @@ common-objs-y = $(addprefix ../../../virt/kvm/, kvm_main.o coalesced_mmio.o even
 EXTRA_CFLAGS += -Ivirt/kvm -Iarch/mips/kvm
 
 common-objs-$(CONFIG_CPU_HAS_MSA) += msa.o
+common-objs-$(CONFIG_CPU_HAS_ASX) += asx.o
 
 kvm-objs := $(common-objs-y) mips.o emulate.o entry.o \
 	    interrupt.o stats.o commpage.o \
diff --git a/arch/mips/kvm/asx.S b/arch/mips/kvm/asx.S
new file mode 100644
index 000000000000..ee855d10b9b8
--- /dev/null
+++ b/arch/mips/kvm/asx.S
@@ -0,0 +1,189 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Loongson Advanced SIMD Extension (ASX) context handling code for KVM.
+ *
+ * Copyright (C) 2019 Loongson Inc.
+ * Author: Xing Li, lixing@loongson.cn
+ * Author: Huacai Chen, chenhc@lemote.com
+ */
+
+#include <asm/asm.h>
+#include <asm/asm-offsets.h>
+#include <asm/asmmacro.h>
+#include <asm/regdef.h>
+
+	.set	noreorder
+	.set	noat
+
+LEAF(__kvm_save_asx)
+	xvst_b	0,  VCPU_FPR0,  a0
+	xvst_b	1,  VCPU_FPR1,  a0
+	xvst_b	2,  VCPU_FPR2,  a0
+	xvst_b	3,  VCPU_FPR3,  a0
+	xvst_b	4,  VCPU_FPR4,  a0
+	xvst_b	5,  VCPU_FPR5,  a0
+	xvst_b	6,  VCPU_FPR6,  a0
+	xvst_b	7,  VCPU_FPR7,  a0
+	xvst_b	8,  VCPU_FPR8,  a0
+	xvst_b	9,  VCPU_FPR9,  a0
+	xvst_b	10, VCPU_FPR10, a0
+	xvst_b	11, VCPU_FPR11, a0
+	xvst_b	12, VCPU_FPR12, a0
+	xvst_b	13, VCPU_FPR13, a0
+	xvst_b	14, VCPU_FPR14, a0
+	xvst_b	15, VCPU_FPR15, a0
+	xvst_b	16, VCPU_FPR16, a0
+	xvst_b	17, VCPU_FPR17, a0
+	xvst_b	18, VCPU_FPR18, a0
+	xvst_b	19, VCPU_FPR19, a0
+	xvst_b	20, VCPU_FPR20, a0
+	xvst_b	21, VCPU_FPR21, a0
+	xvst_b	22, VCPU_FPR22, a0
+	xvst_b	23, VCPU_FPR23, a0
+	xvst_b	24, VCPU_FPR24, a0
+	xvst_b	25, VCPU_FPR25, a0
+	xvst_b	26, VCPU_FPR26, a0
+	xvst_b	27, VCPU_FPR27, a0
+	xvst_b	28, VCPU_FPR28, a0
+	xvst_b	29, VCPU_FPR29, a0
+	xvst_b	30, VCPU_FPR30, a0
+	xvst_b	31, VCPU_FPR31, a0
+	jr	ra
+	 nop
+	END(__kvm_save_asx)
+
+LEAF(__kvm_restore_asx)
+	xvld_b	0,  VCPU_FPR0,  a0
+	xvld_b	1,  VCPU_FPR1,  a0
+	xvld_b	2,  VCPU_FPR2,  a0
+	xvld_b	3,  VCPU_FPR3,  a0
+	xvld_b	4,  VCPU_FPR4,  a0
+	xvld_b	5,  VCPU_FPR5,  a0
+	xvld_b	6,  VCPU_FPR6,  a0
+	xvld_b	7,  VCPU_FPR7,  a0
+	xvld_b	8,  VCPU_FPR8,  a0
+	xvld_b	9,  VCPU_FPR9,  a0
+	xvld_b	10, VCPU_FPR10, a0
+	xvld_b	11, VCPU_FPR11, a0
+	xvld_b	12, VCPU_FPR12, a0
+	xvld_b	13, VCPU_FPR13, a0
+	xvld_b	14, VCPU_FPR14, a0
+	xvld_b	15, VCPU_FPR15, a0
+	xvld_b	16, VCPU_FPR16, a0
+	xvld_b	17, VCPU_FPR17, a0
+	xvld_b	18, VCPU_FPR18, a0
+	xvld_b	19, VCPU_FPR19, a0
+	xvld_b	20, VCPU_FPR20, a0
+	xvld_b	21, VCPU_FPR21, a0
+	xvld_b	22, VCPU_FPR22, a0
+	xvld_b	23, VCPU_FPR23, a0
+	xvld_b	24, VCPU_FPR24, a0
+	xvld_b	25, VCPU_FPR25, a0
+	xvld_b	26, VCPU_FPR26, a0
+	xvld_b	27, VCPU_FPR27, a0
+	xvld_b	28, VCPU_FPR28, a0
+	xvld_b	29, VCPU_FPR29, a0
+	xvld_b	30, VCPU_FPR30, a0
+	xvld_b	31, VCPU_FPR31, a0
+	jr	ra
+	 nop
+	END(__kvm_restore_asx)
+
+	.macro	kvm_restore_asx_upper128 wr, off, base
+	.set	push
+	.set	noat
+	ld $1, (\off+8)(\base)
+	xinsert_d \wr, 2
+	ld $1, (\off+16)(\base)
+	xinsert_d \wr, 3
+	.set	pop
+	.endm
+
+	.macro	kvm_restore_asx_upper192 wr, off, base
+	.set	push
+	.set	noat
+	ld $1, \off(\base)
+	xinsert_d \wr, 1
+	ld $1, (\off+8)(\base)
+	xinsert_d \wr, 2
+	ld $1, (\off+16)(\base)
+	xinsert_d \wr, 3
+	.set	pop
+	.endm
+
+LEAF(__kvm_restore_asx_upper128)
+	kvm_restore_asx_upper128	0,  VCPU_FPR0 +8, a0
+	kvm_restore_asx_upper128	1,  VCPU_FPR1 +8, a0
+	kvm_restore_asx_upper128	2,  VCPU_FPR2 +8, a0
+	kvm_restore_asx_upper128	3,  VCPU_FPR3 +8, a0
+	kvm_restore_asx_upper128	4,  VCPU_FPR4 +8, a0
+	kvm_restore_asx_upper128	5,  VCPU_FPR5 +8, a0
+	kvm_restore_asx_upper128	6,  VCPU_FPR6 +8, a0
+	kvm_restore_asx_upper128	7,  VCPU_FPR7 +8, a0
+	kvm_restore_asx_upper128	8,  VCPU_FPR8 +8, a0
+	kvm_restore_asx_upper128	9,  VCPU_FPR9 +8, a0
+	kvm_restore_asx_upper128	10, VCPU_FPR10+8, a0
+	kvm_restore_asx_upper128	11, VCPU_FPR11+8, a0
+	kvm_restore_asx_upper128	12, VCPU_FPR12+8, a0
+	kvm_restore_asx_upper128	13, VCPU_FPR13+8, a0
+	kvm_restore_asx_upper128	14, VCPU_FPR14+8, a0
+	kvm_restore_asx_upper128	15, VCPU_FPR15+8, a0
+	kvm_restore_asx_upper128	16, VCPU_FPR16+8, a0
+	kvm_restore_asx_upper128	17, VCPU_FPR17+8, a0
+	kvm_restore_asx_upper128	18, VCPU_FPR18+8, a0
+	kvm_restore_asx_upper128	19, VCPU_FPR19+8, a0
+	kvm_restore_asx_upper128	20, VCPU_FPR20+8, a0
+	kvm_restore_asx_upper128	21, VCPU_FPR21+8, a0
+	kvm_restore_asx_upper128	22, VCPU_FPR22+8, a0
+	kvm_restore_asx_upper128	23, VCPU_FPR23+8, a0
+	kvm_restore_asx_upper128	24, VCPU_FPR24+8, a0
+	kvm_restore_asx_upper128	25, VCPU_FPR25+8, a0
+	kvm_restore_asx_upper128	26, VCPU_FPR26+8, a0
+	kvm_restore_asx_upper128	27, VCPU_FPR27+8, a0
+	kvm_restore_asx_upper128	28, VCPU_FPR28+8, a0
+	kvm_restore_asx_upper128	29, VCPU_FPR29+8, a0
+	kvm_restore_asx_upper128	30, VCPU_FPR30+8, a0
+	kvm_restore_asx_upper128	31, VCPU_FPR31+8, a0
+	jr	ra
+	 nop
+	END(__kvm_restore_asx_upper128)
+
+LEAF(__kvm_restore_asx_upper192)
+	kvm_restore_asx_upper192	0,  VCPU_FPR0 +8, a0
+	kvm_restore_asx_upper192	1,  VCPU_FPR1 +8, a0
+	kvm_restore_asx_upper192	2,  VCPU_FPR2 +8, a0
+	kvm_restore_asx_upper192	3,  VCPU_FPR3 +8, a0
+	kvm_restore_asx_upper192	4,  VCPU_FPR4 +8, a0
+	kvm_restore_asx_upper192	5,  VCPU_FPR5 +8, a0
+	kvm_restore_asx_upper192	6,  VCPU_FPR6 +8, a0
+	kvm_restore_asx_upper192	7,  VCPU_FPR7 +8, a0
+	kvm_restore_asx_upper192	8,  VCPU_FPR8 +8, a0
+	kvm_restore_asx_upper192	9,  VCPU_FPR9 +8, a0
+	kvm_restore_asx_upper192	10, VCPU_FPR10+8, a0
+	kvm_restore_asx_upper192	11, VCPU_FPR11+8, a0
+	kvm_restore_asx_upper192	12, VCPU_FPR12+8, a0
+	kvm_restore_asx_upper192	13, VCPU_FPR13+8, a0
+	kvm_restore_asx_upper192	14, VCPU_FPR14+8, a0
+	kvm_restore_asx_upper192	15, VCPU_FPR15+8, a0
+	kvm_restore_asx_upper192	16, VCPU_FPR16+8, a0
+	kvm_restore_asx_upper192	17, VCPU_FPR17+8, a0
+	kvm_restore_asx_upper192	18, VCPU_FPR18+8, a0
+	kvm_restore_asx_upper192	19, VCPU_FPR19+8, a0
+	kvm_restore_asx_upper192	20, VCPU_FPR20+8, a0
+	kvm_restore_asx_upper192	21, VCPU_FPR21+8, a0
+	kvm_restore_asx_upper192	22, VCPU_FPR22+8, a0
+	kvm_restore_asx_upper192	23, VCPU_FPR23+8, a0
+	kvm_restore_asx_upper192	24, VCPU_FPR24+8, a0
+	kvm_restore_asx_upper192	25, VCPU_FPR25+8, a0
+	kvm_restore_asx_upper192	26, VCPU_FPR26+8, a0
+	kvm_restore_asx_upper192	27, VCPU_FPR27+8, a0
+	kvm_restore_asx_upper192	28, VCPU_FPR28+8, a0
+	kvm_restore_asx_upper192	29, VCPU_FPR29+8, a0
+	kvm_restore_asx_upper192	30, VCPU_FPR30+8, a0
+	kvm_restore_asx_upper192	31, VCPU_FPR31+8, a0
+	jr	ra
+	 nop
+	END(__kvm_restore_asx_upper192)
diff --git a/arch/mips/kvm/entry.c b/arch/mips/kvm/entry.c
index 05a85343b92a..3bd79d94820d 100644
--- a/arch/mips/kvm/entry.c
+++ b/arch/mips/kvm/entry.c
@@ -66,6 +66,7 @@
 #define C0_EPC		14, 0
 #define C0_EBASE	15, 1
 #define C0_CONFIG5	16, 5
+#define C0_DIAG1	22, 1
 #define C0_DDATA_LO	28, 3
 #define C0_ERROREPC	30, 0
 
@@ -676,6 +677,9 @@ void *kvm_mips_build_exit(void *addr)
 	uasm_i_mfc0(&p, K0, C0_CAUSE);
 	uasm_i_sw(&p, K0, offsetof(struct kvm_vcpu_arch, host_cp0_cause), K1);
 
+	uasm_i_mfc0(&p, K0, C0_DIAG1);
+	uasm_i_sw(&p, K0, offsetof(struct kvm_vcpu_arch, host_cp0_diag1), K1);
+
 	if (cpu_has_badinstr) {
 		uasm_i_mfc0(&p, K0, C0_BADINSTR);
 		uasm_i_sw(&p, K0, offsetof(struct kvm_vcpu_arch,
diff --git a/arch/mips/kvm/mips.c b/arch/mips/kvm/mips.c
index db185fbd85ef..6918efe7838d 100644
--- a/arch/mips/kvm/mips.c
+++ b/arch/mips/kvm/mips.c
@@ -58,6 +58,7 @@ struct kvm_stats_debugfs_item debugfs_entries[] = {
 	{ "msa_fpe",	  VCPU_STAT(msa_fpe_exits),	 KVM_STAT_VCPU },
 	{ "fpe",	  VCPU_STAT(fpe_exits),		 KVM_STAT_VCPU },
 	{ "msa_disabled", VCPU_STAT(msa_disabled_exits), KVM_STAT_VCPU },
+	{ "asx_disabled", VCPU_STAT(asx_disabled_exits), KVM_STAT_VCPU },
 	{ "flush_dcache", VCPU_STAT(flush_dcache_exits), KVM_STAT_VCPU },
 #ifdef CONFIG_KVM_MIPS_VZ
 	{ "vz_gpsi",	  VCPU_STAT(vz_gpsi_exits),	 KVM_STAT_VCPU },
@@ -598,8 +599,11 @@ static unsigned long kvm_mips_num_regs(struct kvm_vcpu *vcpu)
 		if (boot_cpu_data.fpu_id & MIPS_FPIR_F64)
 			ret += 16;
 	}
-	if (kvm_mips_guest_can_have_msa(&vcpu->arch))
+	if (kvm_mips_guest_can_have_msa(&vcpu->arch)) {
 		ret += ARRAY_SIZE(kvm_mips_get_one_regs_msa) + 32;
+		if (kvm_mips_guest_has_asx(&vcpu->arch))
+			ret += 32;
+	}
 	ret += kvm_mips_callbacks->num_regs(vcpu);
 
 	return ret;
@@ -645,7 +649,10 @@ static int kvm_mips_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *indices)
 		indices += ARRAY_SIZE(kvm_mips_get_one_regs_msa);
 
 		for (i = 0; i < 32; ++i) {
-			index = KVM_REG_MIPS_VEC_128(i);
+			if (kvm_mips_guest_has_asx(&vcpu->arch))
+				index = KVM_REG_MIPS_VEC_256(i);
+			else
+				index = KVM_REG_MIPS_VEC_128(i);
 			if (copy_to_user(indices, &index, sizeof(index)))
 				return -EFAULT;
 			++indices;
@@ -663,6 +670,7 @@ static int kvm_mips_get_reg(struct kvm_vcpu *vcpu,
 	int ret;
 	s64 v;
 	s64 vs[2];
+	s64 avs[4];
 	unsigned int idx;
 
 	switch (reg->id) {
@@ -729,6 +737,28 @@ static int kvm_mips_get_reg(struct kvm_vcpu *vcpu,
 		/* most significant byte first */
 		vs[0] = get_fpr64(&fpu->fpr[idx], 1);
 		vs[1] = get_fpr64(&fpu->fpr[idx], 0);
+#endif
+		break;
+	/* LOONGSON SIMD Architecture Extention (ASX) registers */
+	case KVM_REG_MIPS_VEC_256(0) ... KVM_REG_MIPS_VEC_256(31):
+		if (!kvm_mips_guest_has_asx(&vcpu->arch))
+			return -EINVAL;
+		/* Can't access MSA registers in FR=0 mode */
+		if (!(kvm_read_c0_guest_status(cop0) & ST0_FR))
+			return -EINVAL;
+		idx = reg->id - KVM_REG_MIPS_VEC_256(0);
+#ifdef CONFIG_CPU_LITTLE_ENDIAN
+		/* least significant byte first */
+		avs[0] = get_fpr64(&fpu->fpr[idx], 0);
+		avs[1] = get_fpr64(&fpu->fpr[idx], 1);
+		avs[2] = get_fpr64(&fpu->fpr[idx], 2);
+		avs[3] = get_fpr64(&fpu->fpr[idx], 3);
+#else
+		/* most significant byte first */
+		avs[0] = get_fpr64(&fpu->fpr[idx], 3);
+		avs[1] = get_fpr64(&fpu->fpr[idx], 2);
+		avs[2] = get_fpr64(&fpu->fpr[idx], 1);
+		avs[3] = get_fpr64(&fpu->fpr[idx], 0);
 #endif
 		break;
 	case KVM_REG_MIPS_MSA_IR:
@@ -762,6 +792,10 @@ static int kvm_mips_get_reg(struct kvm_vcpu *vcpu,
 		void __user *uaddr = (void __user *)(long)reg->addr;
 
 		return copy_to_user(uaddr, vs, 16) ? -EFAULT : 0;
+	} else if ((reg->id & KVM_REG_SIZE_MASK) == KVM_REG_SIZE_U256) {
+		void __user *uaddr = (void __user *)(long)reg->addr;
+
+		return copy_to_user(uaddr, avs, 32) ? -EFAULT : 0;
 	} else {
 		return -EINVAL;
 	}
@@ -774,6 +808,7 @@ static int kvm_mips_set_reg(struct kvm_vcpu *vcpu,
 	struct mips_fpu_struct *fpu = &vcpu->arch.fpu;
 	s64 v;
 	s64 vs[2];
+	s64 avs[4];
 	unsigned int idx;
 
 	if ((reg->id & KVM_REG_SIZE_MASK) == KVM_REG_SIZE_U64) {
@@ -792,6 +827,10 @@ static int kvm_mips_set_reg(struct kvm_vcpu *vcpu,
 		void __user *uaddr = (void __user *)(long)reg->addr;
 
 		return copy_from_user(vs, uaddr, 16) ? -EFAULT : 0;
+	} else if ((reg->id & KVM_REG_SIZE_MASK) == KVM_REG_SIZE_U256) {
+		void __user *uaddr = (void __user *)(long)reg->addr;
+
+		return copy_from_user(avs, uaddr, 32) ? -EFAULT : 0;
 	} else {
 		return -EINVAL;
 	}
@@ -860,6 +899,25 @@ static int kvm_mips_set_reg(struct kvm_vcpu *vcpu,
 		/* most significant byte first */
 		set_fpr64(&fpu->fpr[idx], 1, vs[0]);
 		set_fpr64(&fpu->fpr[idx], 0, vs[1]);
+#endif
+		break;
+	/* LOONGSON SIMD Architecture Extention (ASX) registers */
+	case KVM_REG_MIPS_VEC_256(0) ... KVM_REG_MIPS_VEC_256(31):
+		if (!kvm_mips_guest_has_asx(&vcpu->arch))
+			return -EINVAL;
+		idx = reg->id - KVM_REG_MIPS_VEC_256(0);
+#ifdef CONFIG_CPU_LITTLE_ENDIAN
+		/* least significant byte first */
+		set_fpr64(&fpu->fpr[idx], 0, avs[0]);
+		set_fpr64(&fpu->fpr[idx], 1, avs[1]);
+		set_fpr64(&fpu->fpr[idx], 2, avs[2]);
+		set_fpr64(&fpu->fpr[idx], 3, avs[3]);
+#else
+		/* most significant byte first */
+		set_fpr64(&fpu->fpr[idx], 3, avs[0]);
+		set_fpr64(&fpu->fpr[idx], 2, avs[1]);
+		set_fpr64(&fpu->fpr[idx], 1, avs[2]);
+		set_fpr64(&fpu->fpr[idx], 0, avs[3]);
 #endif
 		break;
 	case KVM_REG_MIPS_MSA_IR:
@@ -899,6 +957,9 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	case KVM_CAP_MIPS_MSA:
 		vcpu->arch.msa_enabled = true;
 		break;
+	case KVM_CAP_MIPS_ASX:
+		vcpu->arch.asx_enabled = true;
+		break;
 	default:
 		r = -EINVAL;
 		break;
@@ -1148,6 +1209,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		 */
 		r = cpu_has_msa && !(boot_cpu_data.msa_id & MSA_IR_WRPF);
 		break;
+	case KVM_CAP_MIPS_ASX:
+		r = cpu_has_asx && !(boot_cpu_data.msa_id & MSA_IR_WRPF);
+		break;
 	default:
 		r = kvm_mips_callbacks->check_extension(kvm, ext);
 		break;
@@ -1295,7 +1359,7 @@ static void kvm_mips_set_c0_status(void)
 int kvm_mips_handle_exit(struct kvm_run *run, struct kvm_vcpu *vcpu)
 {
 	u32 cause = vcpu->arch.host_cp0_cause;
-	u32 exccode = (cause >> CAUSEB_EXCCODE) & 0x1f;
+	u32 exccode = (cause >> CAUSEB_EXCCODE) & 0x1f, gsexccode;
 	u32 __user *opc = (u32 __user *) vcpu->arch.pc;
 	unsigned long badvaddr = vcpu->arch.host_cp0_badvaddr;
 	enum emulation_result er = EMULATE_DONE;
@@ -1434,6 +1498,17 @@ int kvm_mips_handle_exit(struct kvm_run *run, struct kvm_vcpu *vcpu)
 		ret = kvm_mips_callbacks->handle_guest_exit(vcpu);
 		break;
 
+	case LOONGSON_EXCCODE_GSEXC:
+		gsexccode = (vcpu->arch.host_cp0_diag1 & LOONGSON_DIAG1_EXCCODE) >> LOONGSON_DIAG1_EXCCODE_SHIFT;
+
+		if (gsexccode == GSEXCCODE_LASXDIS) {
+			++vcpu->stat.asx_disabled_exits;
+			ret = kvm_mips_callbacks->handle_asx_disabled(vcpu);
+		} else {
+			kvm_err("Unhandled GSEXC %x @ %lx\n", gsexccode, vcpu->arch.pc);
+		}
+		break;
+
 	default:
 		if (cause & CAUSEF_BD)
 			opc += 1;
@@ -1527,7 +1602,7 @@ void kvm_own_fpu(struct kvm_vcpu *vcpu)
 	 * not to clobber the status register directly via the commpage.
 	 */
 	if (cpu_has_msa && sr & ST0_CU1 && !(sr & ST0_FR) &&
-	    vcpu->arch.aux_inuse & KVM_MIPS_AUX_MSA)
+	    vcpu->arch.aux_inuse & (KVM_MIPS_AUX_MSA | KVM_MIPS_AUX_ASX))
 		kvm_lose_fpu(vcpu);
 
 	/*
@@ -1575,7 +1650,7 @@ void kvm_own_msa(struct kvm_vcpu *vcpu)
 		 */
 		if (!(sr & ST0_FR) &&
 		    (vcpu->arch.aux_inuse & (KVM_MIPS_AUX_FPU |
-				KVM_MIPS_AUX_MSA)) == KVM_MIPS_AUX_FPU)
+			KVM_MIPS_AUX_MSA | KVM_MIPS_AUX_ASX)) == KVM_MIPS_AUX_FPU)
 			kvm_lose_fpu(vcpu);
 
 		change_c0_status(ST0_CU1 | ST0_FR, sr);
@@ -1589,7 +1664,8 @@ void kvm_own_msa(struct kvm_vcpu *vcpu)
 	set_c0_config5(MIPS_CONF5_MSAEN);
 	enable_fpu_hazard();
 
-	switch (vcpu->arch.aux_inuse & (KVM_MIPS_AUX_FPU | KVM_MIPS_AUX_MSA)) {
+	switch (vcpu->arch.aux_inuse &
+			(KVM_MIPS_AUX_FPU | KVM_MIPS_AUX_MSA | KVM_MIPS_AUX_ASX)) {
 	case KVM_MIPS_AUX_FPU:
 		/*
 		 * Guest FPU state already loaded, only restore upper MSA state
@@ -1612,6 +1688,110 @@ void kvm_own_msa(struct kvm_vcpu *vcpu)
 		break;
 	}
 
+#ifdef CONFIG_CPU_HAS_ASX
+	/* Check if the guest context whether ASX or not,
+	 * if ASX state live, retsore upper ASX state
+	 */
+	if ((read_gc0_config() & MIPS_CONF_LASXEN)) {
+		set_c0_config(MIPS_CONF_LASXEN);
+		__kvm_restore_asx_upper128(&vcpu->arch);
+		vcpu->arch.aux_inuse |= KVM_MIPS_AUX_ASX;
+	}
+#endif
+
+	preempt_enable();
+}
+#endif
+
+#ifdef CONFIG_CPU_HAS_ASX
+/* Enable ASX for guest and restore context */
+void kvm_own_asx(struct kvm_vcpu *vcpu)
+{
+	struct mips_coproc *cop0 = vcpu->arch.cop0;
+	unsigned int sr;
+
+	preempt_disable();
+
+	/*
+	 * Enable FPU if enabled in guest, since we're restoring FPU context
+	 * anyway. We set FR and FRE according to guest context.
+	 */
+	if (kvm_mips_guest_has_msa(&vcpu->arch)) {
+		sr = kvm_read_c0_guest_status(cop0);
+		/*
+		 * If FR=0 FPU state is already live, it is undefined how it
+		 * interacts with 256 vector state, so play it safe and save
+		 * it first.
+		 */
+		if (!(sr & ST0_FR) &&
+		((vcpu->arch.aux_inuse & (KVM_MIPS_AUX_FPU |
+		    KVM_MIPS_AUX_MSA | KVM_MIPS_AUX_ASX)) == KVM_MIPS_AUX_MSA))
+			kvm_lose_fpu(vcpu);
+
+		/* Enable MSA for guest */
+		set_c0_config5(MIPS_CONF5_MSAEN);
+	}
+
+	/*
+	 * Enable FPU if enabled in guest, since we're restoring FPU context
+	 * anyway. We set FR and FRE according to guest context.
+	 */
+	if (kvm_mips_guest_has_fpu(&vcpu->arch)) {
+		sr = kvm_read_c0_guest_status(cop0);
+
+		/*
+		 * If FR=0 FPU state is already live, it is undefined how it
+		 * interacts with MSA state, so play it safe and save it first.
+		 */
+		if (!(sr & ST0_FR) &&
+		    (vcpu->arch.aux_inuse & (KVM_MIPS_AUX_FPU |
+				KVM_MIPS_AUX_MSA | KVM_MIPS_AUX_ASX)) == KVM_MIPS_AUX_FPU)
+			kvm_lose_fpu(vcpu);
+
+		change_c0_status(ST0_CU1 | ST0_FR, sr);
+	}
+
+	/* Enable ASX for guest */
+	set_c0_config(MIPS_CONF_LASXEN);
+	enable_fpu_hazard();
+
+	switch (vcpu->arch.aux_inuse & (KVM_MIPS_AUX_FPU |
+			 KVM_MIPS_AUX_MSA | KVM_MIPS_AUX_ASX)) {
+	case (KVM_MIPS_AUX_MSA | KVM_MIPS_AUX_FPU):
+	case KVM_MIPS_AUX_MSA:
+		/*
+		 * Guest MSA state already loaded, only restore upper LASX state
+		 */
+		__kvm_restore_asx_upper128(&vcpu->arch);
+		vcpu->arch.aux_inuse |= KVM_MIPS_AUX_ASX;
+		trace_kvm_aux(vcpu, KVM_TRACE_AUX_RESTORE, KVM_TRACE_AUX_ASX);
+		break;
+	case KVM_MIPS_AUX_FPU:
+		/*
+		 * Guest FPU state already loaded, only restore 64~256 LASX state
+		 */
+		__kvm_restore_asx_upper192(&vcpu->arch);
+		vcpu->arch.aux_inuse |= KVM_MIPS_AUX_ASX;
+		if (kvm_mips_guest_has_msa(&vcpu->arch))
+			vcpu->arch.aux_inuse |= KVM_MIPS_AUX_MSA;
+		trace_kvm_aux(vcpu, KVM_TRACE_AUX_RESTORE, KVM_TRACE_AUX_ASX);
+		break;
+	case 0:
+		/* Neither FPU or MSA already active, restore full LASX state */
+		__kvm_restore_asx(&vcpu->arch);
+		vcpu->arch.aux_inuse |= KVM_MIPS_AUX_ASX;
+		if (kvm_mips_guest_has_msa(&vcpu->arch))
+			vcpu->arch.aux_inuse |= KVM_MIPS_AUX_MSA;
+		if (kvm_mips_guest_has_fpu(&vcpu->arch))
+			vcpu->arch.aux_inuse |= KVM_MIPS_AUX_FPU;
+		trace_kvm_aux(vcpu, KVM_TRACE_AUX_RESTORE,
+			      KVM_TRACE_AUX_FPU_MSA_ASX);
+		break;
+	default:
+		trace_kvm_aux(vcpu, KVM_TRACE_AUX_ENABLE, KVM_TRACE_AUX_ASX);
+		break;
+	}
+
 	preempt_enable();
 }
 #endif
@@ -1620,6 +1800,11 @@ void kvm_own_msa(struct kvm_vcpu *vcpu)
 void kvm_drop_fpu(struct kvm_vcpu *vcpu)
 {
 	preempt_disable();
+	if (cpu_has_asx && (vcpu->arch.aux_inuse & KVM_MIPS_AUX_ASX)) {
+		disable_asx();
+		trace_kvm_aux(vcpu, KVM_TRACE_AUX_DISCARD, KVM_TRACE_AUX_ASX);
+		vcpu->arch.aux_inuse &= ~KVM_MIPS_AUX_ASX;
+	}
 	if (cpu_has_msa && vcpu->arch.aux_inuse & KVM_MIPS_AUX_MSA) {
 		disable_msa();
 		trace_kvm_aux(vcpu, KVM_TRACE_AUX_DISCARD, KVM_TRACE_AUX_MSA);
@@ -1633,18 +1818,33 @@ void kvm_drop_fpu(struct kvm_vcpu *vcpu)
 	preempt_enable();
 }
 
-/* Save and disable FPU & MSA */
+/* Save and disable FPU & MSA & ASX */
 void kvm_lose_fpu(struct kvm_vcpu *vcpu)
 {
 	/*
-	 * With T&E, FPU & MSA get disabled in root context (hardware) when it
-	 * is disabled in guest context (software), but the register state in
+	 * With T&E, FPU & MSA & ASXget disabled in root context (hardware) when
+	 * it is disabled in guest context (software), but the register state in
 	 * the hardware may still be in use.
 	 * This is why we explicitly re-enable the hardware before saving.
 	 */
 
 	preempt_disable();
-	if (cpu_has_msa && vcpu->arch.aux_inuse & KVM_MIPS_AUX_MSA) {
+	if (cpu_has_asx && (vcpu->arch.aux_inuse & KVM_MIPS_AUX_ASX)) {
+
+		__kvm_save_asx(&vcpu->arch);
+		trace_kvm_aux(vcpu, KVM_TRACE_AUX_SAVE, KVM_TRACE_AUX_FPU_MSA_ASX);
+
+		/* Disable ASX & MAS & FPU */
+		disable_asx();
+		disable_msa();
+
+		if (vcpu->arch.aux_inuse & KVM_MIPS_AUX_FPU) {
+			clear_c0_status(ST0_CU1 | ST0_FR);
+			disable_fpu_hazard();
+		}
+		vcpu->arch.aux_inuse &= ~(KVM_MIPS_AUX_FPU |
+					 KVM_MIPS_AUX_MSA | KVM_MIPS_AUX_ASX);
+	} else if (cpu_has_msa && vcpu->arch.aux_inuse & KVM_MIPS_AUX_MSA) {
 		if (!IS_ENABLED(CONFIG_KVM_MIPS_VZ)) {
 			set_c0_config5(MIPS_CONF5_MSAEN);
 			enable_fpu_hazard();
diff --git a/arch/mips/kvm/trace.h b/arch/mips/kvm/trace.h
index a8c7fd7bf6d2..e08de50b518c 100644
--- a/arch/mips/kvm/trace.h
+++ b/arch/mips/kvm/trace.h
@@ -235,7 +235,9 @@ TRACE_EVENT(kvm_hwr,
 
 #define KVM_TRACE_AUX_FPU		1
 #define KVM_TRACE_AUX_MSA		2
+#define KVM_TRACE_AUX_ASX		4
 #define KVM_TRACE_AUX_FPU_MSA		3
+#define KVM_TRACE_AUX_FPU_MSA_ASX	7
 
 #define kvm_trace_symbol_aux_op		\
 	{ KVM_TRACE_AUX_RESTORE, "restore" },	\
@@ -247,7 +249,9 @@ TRACE_EVENT(kvm_hwr,
 #define kvm_trace_symbol_aux_state		\
 	{ KVM_TRACE_AUX_FPU,     "FPU" },	\
 	{ KVM_TRACE_AUX_MSA,     "MSA" },	\
-	{ KVM_TRACE_AUX_FPU_MSA, "FPU & MSA" }
+	{ KVM_TRACE_AUX_ASX,     "ASX" },	\
+	{ KVM_TRACE_AUX_FPU_MSA, "FPU & MSA" },	\
+	{ KVM_TRACE_AUX_FPU_MSA_ASX, "FPU & MSA & ASX" }
 
 TRACE_EVENT(kvm_aux,
 	    TP_PROTO(struct kvm_vcpu *vcpu, unsigned int op,
diff --git a/arch/mips/kvm/vz.c b/arch/mips/kvm/vz.c
index 94da1a7d6a67..9ff2adac8416 100644
--- a/arch/mips/kvm/vz.c
+++ b/arch/mips/kvm/vz.c
@@ -81,7 +81,11 @@ static inline void kvm_vz_write_gc0_ebase(long v)
 
 static inline unsigned int kvm_vz_config_guest_wrmask(struct kvm_vcpu *vcpu)
 {
+#ifndef CONFIG_CPU_LOONGSON3
 	return CONF_CM_CMASK;
+#else
+	return CONF_CM_CMASK | MIPS_CONF_LASXEN;
+#endif
 }
 
 static inline unsigned int kvm_vz_config1_guest_wrmask(struct kvm_vcpu *vcpu)
@@ -129,7 +133,13 @@ static inline unsigned int kvm_vz_config5_guest_wrmask(struct kvm_vcpu *vcpu)
 
 static inline unsigned int kvm_vz_config6_guest_wrmask(struct kvm_vcpu *vcpu)
 {
-	return MIPS_CONF6_INTIMER | MIPS_CONF6_EXTIMER | MIPS_CONF6_SYND;
+	unsigned int mask = MIPS_CONF6_INTIMER | MIPS_CONF6_EXTIMER | MIPS_CONF6_SYND;
+
+	if (kvm_mips_guest_has_asx(&vcpu->arch))
+		mask |= MIPS_CONF6_LASXMODE;
+
+	return mask;
+
 }
 
 /*
@@ -1171,7 +1181,7 @@ static enum emulation_result kvm_vz_gpsi_lwc2(union mips_instruction inst,
 		case LOONGSON_CFG1:
 			hostcfg &= (LOONGSON_CFG1_FP | LOONGSON_CFG1_MMI |
 				    LOONGSON_CFG1_MSA1 | LOONGSON_CFG1_MSA2 |
-				    LOONGSON_CFG1_SFBP);
+				    LOONGSON_CFG1_LASX | LOONGSON_CFG1_SFBP);
 			vcpu->arch.gprs[rd] = hostcfg;
 			break;
 		case LOONGSON_CFG2:
@@ -1353,7 +1363,7 @@ static enum emulation_result kvm_trap_vz_handle_gsfc(u32 cause, u32 *opc,
 			 * safe and save it first.
 			 */
 			if (change & ST0_CU1 && !(val & ST0_FR) &&
-			    vcpu->arch.aux_inuse & KVM_MIPS_AUX_MSA)
+			    vcpu->arch.aux_inuse & (KVM_MIPS_AUX_MSA | KVM_MIPS_AUX_ASX))
 				kvm_lose_fpu(vcpu);
 
 			write_gc0_status(val);
@@ -1381,6 +1391,13 @@ static enum emulation_result kvm_trap_vz_handle_gsfc(u32 cause, u32 *opc,
 			write_gc0_cause(old_cause ^ change);
 		} else if ((rd == MIPS_CP0_STATUS) && (sel == 1)) { /* IntCtl */
 			write_gc0_intctl(val);
+		} else if ((rd == MIPS_CP0_CONFIG) && (sel == 0)) {
+			/* Handle changes in ASX modes */
+			old_val = read_gc0_config();
+			change = val ^ old_val;
+			val = old_val ^
+				(change & kvm_vz_config_guest_wrmask(vcpu));
+			write_gc0_config5(val);
 		} else if ((rd == MIPS_CP0_CONFIG) && (sel == 5)) {
 			old_val = read_gc0_config5();
 			change = val ^ old_val;
@@ -1617,6 +1634,37 @@ static int kvm_trap_vz_handle_msa_disabled(struct kvm_vcpu *vcpu)
 	return RESUME_GUEST;
 }
 
+/**
+ * kvm_trap_vz_handle_asx_disabled() - Guest used ASX while disabled in root.
+ * @vcpu:	Virtual CPU context.
+ *
+ * Handle when the guest attempts to use ASX when it is disabled in the root
+ * context.
+ */
+static int kvm_trap_vz_handle_asx_disabled(struct kvm_vcpu *vcpu)
+{
+	struct kvm_run *run = vcpu->run;
+
+	/*
+	 * If ASX not present or not exposed to guest or FR=0, the ASX operation
+	 * should have been treated as a reserved instruction!
+	 * Same if CU1=1, FR=0.
+	 * If ASX already in use, we shouldn't get this at all.
+	 */
+	if (!kvm_mips_guest_has_asx(&vcpu->arch) ||
+	    (read_gc0_status() & (ST0_CU1 | ST0_FR)) == ST0_CU1 ||
+	    !(read_gc0_config() & MIPS_CONF_LASXEN) ||
+	    !(read_gc0_config5() & MIPS_CONF5_MSAEN) ||
+	    vcpu->arch.aux_inuse & KVM_MIPS_AUX_ASX) {
+		run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+		return RESUME_HOST;
+	}
+
+	kvm_own_asx(vcpu);
+
+	return RESUME_GUEST;
+}
+
 static int kvm_trap_vz_handle_tlb_ld_miss(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
@@ -3283,6 +3331,7 @@ static struct kvm_mips_callbacks kvm_vz_callbacks = {
 	.handle_res_inst = kvm_trap_vz_no_handler,
 	.handle_break = kvm_trap_vz_no_handler,
 	.handle_msa_disabled = kvm_trap_vz_handle_msa_disabled,
+	.handle_asx_disabled = kvm_trap_vz_handle_asx_disabled,
 	.handle_guest_exit = kvm_trap_vz_handle_guest_exit,
 
 	.hardware_enable = kvm_vz_hardware_enable,
diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
index 34c02e4290fe..01db0e498685 100644
--- a/include/uapi/linux/elf.h
+++ b/include/uapi/linux/elf.h
@@ -428,6 +428,7 @@ typedef struct elf64_shdr {
 #define NT_MIPS_DSP	0x800		/* MIPS DSP ASE registers */
 #define NT_MIPS_FP_MODE	0x801		/* MIPS floating-point mode */
 #define NT_MIPS_MSA	0x802		/* MIPS SIMD registers */
+#define NT_MIPS_ASX	0x803		/* Loongson SIMD registers */
 
 /* Note header in a PT_NOTE section */
 typedef struct elf32_note {
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 1b6b8e05868d..d50778388e79 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -1003,6 +1003,7 @@ struct kvm_ppc_resize_hpt {
 #define KVM_CAP_PMU_EVENT_FILTER 173
 #define KVM_CAP_ARM_IRQ_LINE_LAYOUT_2 174
 #define KVM_CAP_HYPERV_DIRECT_TLBFLUSH 175
+#define KVM_CAP_MIPS_ASX 187
 
 #ifdef KVM_CAP_IRQ_ROUTING
 
-- 
2.31.1

